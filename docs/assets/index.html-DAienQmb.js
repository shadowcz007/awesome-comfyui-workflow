import{_ as d,r as l,o as i,c,a as e,b as o,d as p,w as a,e as n}from"./app-BGqkLjHw.js";const r="/awesome-comfyui-workflow/assets/DiffusersLoader-mzdmk8hT.svg",h="/awesome-comfyui-workflow/assets/LoadCheckpointWithConfig-BN6zB4MN.svg",m="/awesome-comfyui-workflow/assets/ApplyControlNet-T6FeDJLs.svg",g="/awesome-comfyui-workflow/assets/ApplyStyleModel-BjkUEVpq.svg",u="/awesome-comfyui-workflow/assets/CLIPSetLastLayer-B9W3GWtU.svg",f="/awesome-comfyui-workflow/assets/CLIPTextEncodePrompt-hZy3NNU2.svg",k="/awesome-comfyui-workflow/assets/CLIPVisionEncode-CftDdXV-.svg",L="/awesome-comfyui-workflow/assets/ConditioningAverage-Drh7U2UB.svg",I="/awesome-comfyui-workflow/assets/ConditioningCombine-C3NkkjHK.svg",b="/awesome-comfyui-workflow/assets/ConditioningSetArea-dVMUIFKq.svg",v="/awesome-comfyui-workflow/assets/ConditioningSetMask-VXvrA2C9.svg",w="/awesome-comfyui-workflow/assets/GLIGENTextboxApply-D4bHz8Hm.svg",_="/awesome-comfyui-workflow/assets/unCLIPConditioning-DLHXFnff.svg",C="/awesome-comfyui-workflow/assets/LoadLatent-C85_Mtz4.svg",x="/awesome-comfyui-workflow/assets/SaveLatent-DtsgIWDD.svg",T="/awesome-comfyui-workflow/assets/TomePatchModel-BBDtRE1G.svg",y="/awesome-comfyui-workflow/assets/TokenMergingforStableDiffusion-CqqHudlC.jpg",E="/awesome-comfyui-workflow/assets/VAEDecodeTiled-DY9VBdS9.svg",N="/awesome-comfyui-workflow/assets/VAEEncodeTiled-BQJoK_t9.svg",M="/awesome-comfyui-workflow/assets/LoadImage-CIzP9BtT.svg",V="data:image/svg+xml;base64,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",U="/awesome-comfyui-workflow/assets/PadImageForOutpainting-Dzr-CDCB.svg",A="/awesome-comfyui-workflow/assets/PreviewImage-Br5eeeHl.svg",S="/awesome-comfyui-workflow/assets/SaveImage-z3yAym7e.svg",P="/awesome-comfyui-workflow/assets/ImageBlend-CAd_GaLc.svg",R="/awesome-comfyui-workflow/assets/ImageBlur-BZL9jSkN.svg",F="/awesome-comfyui-workflow/assets/ImageQuantize-WjbyHjQe.svg",Z="/awesome-comfyui-workflow/assets/ImageSharpen-CAocoy-2.svg",Q="/awesome-comfyui-workflow/assets/UpscaleImage-CCa7VY0g.svg",B="/awesome-comfyui-workflow/assets/UpscaleImageUsingModel-DV-SF4Wg.svg",D="/awesome-comfyui-workflow/assets/EmptyLatentImage-D8EMT608.svg",W="/awesome-comfyui-workflow/assets/LatentComposite-UQ81jvlz.svg",G="/awesome-comfyui-workflow/assets/LatentCompositeMasked-DZJAVyUF.svg",z="/awesome-comfyui-workflow/assets/UpscaleLatent-I-gJiwEn.svg",O="/awesome-comfyui-workflow/assets/VAEDecode-B-picWPt.svg",j="/awesome-comfyui-workflow/assets/VAEEncode-D5EYZEJ2.svg",H="/awesome-comfyui-workflow/assets/LatentFromBatch-YeN1b-Lj.svg",J="/awesome-comfyui-workflow/assets/RebatchLatents-YWT5gNwM.svg",Y="/awesome-comfyui-workflow/assets/RepeatLatentBatch-BqPx0NXv.svg",X="/awesome-comfyui-workflow/assets/SetLatentNoiseMask-MmkLdInd.svg",q="/awesome-comfyui-workflow/assets/VAEEncodeForInpainting-DD-YX7G1.svg",K="/awesome-comfyui-workflow/assets/CropLatent-B_cR3_Xi.svg",$="/awesome-comfyui-workflow/assets/FlipLatent-DBtA-SV6.svg",ee="/awesome-comfyui-workflow/assets/RotateLatent-CFLvhpt7.svg",oe="/awesome-comfyui-workflow/assets/GLIGENLoader-D-vTl3Sj.svg",pe="/awesome-comfyui-workflow/assets/HypernetworkLoader-C8B_JGiF.svg",te="/awesome-comfyui-workflow/assets/LoadCheckpoint-BQouw9PX.svg",ae="/awesome-comfyui-workflow/assets/LoadCLIP-C94A2T08.svg",ne="/awesome-comfyui-workflow/assets/LoadCLIPVision-CtFF4wbm.svg",se="/awesome-comfyui-workflow/assets/LoadControlNet-DHg5P2Us.svg",le="/awesome-comfyui-workflow/assets/LoadLoRA-Bng2EN8f.svg",de="/awesome-comfyui-workflow/assets/LoadStyleModel-cphcEIxJ.svg",ie="/awesome-comfyui-workflow/assets/LoadUpscaleModel-C74PIl6V.svg",ce="/awesome-comfyui-workflow/assets/LoadVAE-2X0vkEPY.svg",re="/awesome-comfyui-workflow/assets/unCLIPCheckpointLoader-ii62sfkE.svg",he="/awesome-comfyui-workflow/assets/ConvertImageToMask-cXPU6ybK.svg",me="/awesome-comfyui-workflow/assets/CropMask-9iFwdALP.svg",ge="/awesome-comfyui-workflow/assets/FeatherMask-CFIc33fP.svg",ue="/awesome-comfyui-workflow/assets/InvertMask-CXALs9bU.svg",fe="/awesome-comfyui-workflow/assets/LoadImageAsMask-CvgYUE6N.svg",ke="/awesome-comfyui-workflow/assets/MaskComposite-D-LgFTnL.svg",Le="/awesome-comfyui-workflow/assets/SolidMask-Ch9FCV-j.svg",Ie="/awesome-comfyui-workflow/assets/KSampler-LOr6cHLj.svg",be="/awesome-comfyui-workflow/assets/KSamplerADV-0c9yINrV.svg",ve={},we=n('<h1 id="核心节点" tabindex="-1"><a class="header-anchor" href="#核心节点"><span>核心节点</span></a></h1><h2 id="扩散模型加载器" tabindex="-1"><a class="header-anchor" href="#扩散模型加载器"><span>扩散模型加载器</span></a></h2><blockquote><p>Diffusers Loader节点（扩散模型加载器），可用于加载扩散模型。</p></blockquote><p><img src="'+r+'" alt="图片"></p><p><strong>输入</strong></p><ul><li><code>model_path</code>：扩散器模型的路径</li></ul><p><strong>输出</strong></p><ul><li><p><code>MODEL</code>：用于去噪潜变量的模型。</p></li><li><p><code>CLIP</code>：用于编码文本提示的CLIP模型。</p></li><li><p><code>VAE</code>：用于将图像编码和解码到潜空间的VAE模型。</p></li></ul><br><h2 id="加载检查点节点" tabindex="-1"><a class="header-anchor" href="#加载检查点节点"><span>加载检查点节点</span></a></h2><blockquote><p>Load Checkpoint (With Config) 节点，可用于根据提供的配置文件加载扩散模型。请注意，通常情况下，常规的Checkpoint能够自动检测出适当的配置。</p></blockquote><p><img src="'+h+'" alt="图片"></p><p><strong>输入</strong></p><ul><li><p><code>config_name</code>：配置文件的名称。</p></li><li><p><code>ckpt_name</code>：要加载的模型的名称。</p></li></ul><p><strong>输出</strong></p><ul><li><p><code>MODEL</code>：用于去噪潜变量的模型。</p></li><li><p><code>CLIP</code>：用于编码文本提示的CLIP模型。</p></li><li><p><code>VAE</code>：用于将图像编码和解码到潜空间的VAE模型。</p></li></ul><br><h2 id="条件设定" tabindex="-1"><a class="header-anchor" href="#条件设定"><span>条件设定</span></a></h2><blockquote><p>在ComfyUI中，条件设定用于指导扩散模型生成特定的输出。所有的条件设定都以由CLIP使用Clip Text Encode节点嵌入的文本提示开始。</p></blockquote><blockquote><p>这些条件可以通过该部分其他节点的进一步增强或修改。例如，使用Conditioning (Set Area)、Conditioning (Set Mask)或GLIGEN Textbox Apply节点来引导进程朝着特定的构图方向发展。</p></blockquote><blockquote><p>或者通过Apply Style Model、应用ControlNet或 unCLIP Conditioning 节点来提供额外的视觉提示。相关节点的完整列表可以在侧边栏中找到。</p></blockquote><h2 id="应用controlnet模型-apply-controlnet" tabindex="-1"><a class="header-anchor" href="#应用controlnet模型-apply-controlnet"><span>应用ControlNet模型 Apply ControlNet</span></a></h2><blockquote><p>Apply ControlNet节点，可以用于为扩散模型提供进一步的视觉指导。与unCLIP嵌入不同，controlnets 和 T2IAdaptor 适用于任何模型。</p></blockquote><p><img src="'+m+'" alt="图片"></p><blockquote><p>通过将多个节点链接在一起，可以使用多个 controlNet 或 T2IAdaptor 来指导扩散模型。例如，可以通过向此节点提供包含边缘检测的图像以及在边缘检测图像上训练的controlNet来提示扩散模型。</p></blockquote><p><strong>输入</strong></p><ul><li><p><code>conditioning</code>:一个条件</p></li><li><p><code>control_net</code>: control_net模型</p></li><li><p><code>image</code>:用作扩散模型的视觉指导</p></li></ul><p><strong>输出</strong></p><ul><li><code>CONDITIONING</code>:一个包含control_net和视觉指导的条件。</li></ul><blockquote><p>提示：要使用T2IAdaptor样式模型，请改用Apply Style Model节点。</p></blockquote><br><h2 id="应用风格模型" tabindex="-1"><a class="header-anchor" href="#应用风格模型"><span>应用风格模型</span></a></h2><blockquote><p>Apply Style Model节点是一个用于为扩散模型提供视觉指导的节点，特别是针对所生成图像的样式。该节点使用T2IAdaptor 模型和来自 CLIP_vision 模型的嵌入，将扩散模型引导到与 CLIP_vision 嵌入图像的样式相符的方向。</p></blockquote><p><img src="'+g+'" alt="图片"></p><p><strong>输入</strong></p><ul><li><p><code>conditioning</code>:一个条件</p></li><li><p><code>style_model</code>:一个T2I样式适配器</p></li><li><p><code>CLIP_vision_output</code>:包含所需样式的图像，由CLIP视觉模型编码</p></li></ul><p><strong>输出</strong></p><ul><li><code>CONDITIONING</code>:包含T2I样式适配器和指向所需样式的视觉指导的条件</li></ul><br><h2 id="设置clip最后一层" tabindex="-1"><a class="header-anchor" href="#设置clip最后一层"><span>设置CLIP最后一层</span></a></h2><blockquote><p>设置CLIP最后一层，CLIP Set Last Layer节点可以用于设置从中获取文本嵌入的CLIP输出层。将文本编码为嵌入是通过将文本通过CLIP模型中的各个层进行转换来实现的。尽管传统上扩散模型是根据CLIP的最后一层的输出进行条件化的，但某些扩散模型是根据较早的层进行条件化的，当使用最后一层的输出时可能效果不佳。</p></blockquote><p><img src="'+u+'" alt="CLIP Set Last Layer"></p><p><strong>输入</strong></p><ul><li><code>clip</code>: 用于编码文本的CLIP模型。</li></ul><p><strong>输出</strong></p><ul><li><code>CLIP</code>: 设置了新的输出层的CLIP模型。</li></ul><blockquote><p>如何理解Clip set last layer的原理？CLIP模型是由OpenAI开发的强大深度学习模型，它结合了视觉和语言理解。它能够以多模态方式理解和生成文本和图像。CLIP由多个层级组成，每个层级比前一个层级更具体。 ​</p></blockquote><blockquote><p>Clip set last layer 指的是在CLIP模型中在较早的层级停止信息流动，而不是一直到最后一层。通过这样做，您可以控制生成的文本描述的具体程度或准确性。例如，如果您正在寻找一张“牛”的图片，您可能对文本模型能够生成的子类别或具体类型的牛不感兴趣，比如“阿伯丁安格斯公牛”。 ​</p></blockquote><blockquote><p>使用 Clip set last layer 的好处在于，它允许您根据特定需求定制生成的文本描述的详细程度。根据应用或任务的不同，您可能希望在某个层级停止，以达到所需的准确性或相关性。例如，如果您有一个关于一个年轻人站在田野上的详细提示，使用较低的CLIP skip层级可能会生成诸如“一个站着的人”，“站着的年轻人”或“站在森林中的年轻人”等描述，每个描述都具有不同的特定程度。 ​</p></blockquote><blockquote><p>值得注意的是，Clip set last layer 在与特定方式结构化的模型（如Booru模型）一起使用时特别有用。这些模型通常具有可以分解为多个子标签的标签，从而可以更精细地控制生成的描述。然而，CLIP skip的效果可能因具体的模型和应用而异，可能需要一些试错才能找到最佳设置。 ​</p></blockquote>',50),_e={href:"https://t.zsxq.com/13XWLqc5r",target:"_blank",rel:"noopener noreferrer"},Ce=e("h2",{id:"文本提示",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#文本提示"},[e("span",null,"文本提示")])],-1),xe=e("blockquote",null,[e("p",null,"输入Prompt（文本提示），CLIP文本编码节点将使用CLIP模型对文本提示进行编码，生成一个嵌入向量，用来指导扩散模型生成特定的图像。")],-1),Te=e("p",null,[e("img",{src:f,alt:"图片"})],-1),ye={href:"https://blenderneko.github.io/ComfyUI-docs/Interface/Textprompts/",target:"_blank",rel:"noopener noreferrer"},Ee=n('<p><strong>输入</strong></p><ul><li><p><code>clip</code> - 用于编码文本的CLIP模型</p></li><li><p><code>text</code> - 要编码的文本。</p></li></ul><p><strong>输出</strong></p><ul><li><code>CONDITIONING</code> - 包含嵌入文本的条件，用于指导扩散模型。</li></ul><h2 id="视觉编码" tabindex="-1"><a class="header-anchor" href="#视觉编码"><span>视觉编码</span></a></h2><p><img src="'+k+'" alt="CLIP Vision Encode node"></p><p>CLIP Vision Encode节点可以使用CLIP视觉模型对图像进行编码，生成可用于指导 unCLIP 扩散模型或作为样式模型输入的嵌入。</p><p><strong>输入</strong></p><p><code>clip_vision</code></p><p>: 用于编码图像的CLIP视觉模型</p><p><code>image</code></p><p>: 待编码的图像.</p><p><strong>输出</strong></p><p><code>CLIP_VISION_OUTPUT</code></p><p>: 编码后的图像。</p><h2 id="平均条件化" tabindex="-1"><a class="header-anchor" href="#平均条件化"><span>平均条件化</span></a></h2><p><img src="'+L+'" alt="Conditioning (Average) node"></p><p>条件化平均节点，Conditioning (Average) 节点可以根据在 conditioning_to_strength 中设置的强度因子，在两个文本嵌入之间进行插值。</p><p><strong>输入</strong></p><p><code>conditioning_to</code></p><p>: 在 conditioning_to_strength 为1时的文本嵌入的条件化</p><p><code>conditioning_from</code></p><p>: 在 conditioning_to_strength 为0时的文本嵌入的条件化。</p><p><code>conditioning_to_strength</code></p><p>: 混合 conditioning_to 到 conditioning_from 的因子。</p><p><strong>输出</strong></p><p><code>CONDITIONING</code></p><p>: 基于 conditioning_to_strength 混合的文本嵌入的新条件化。</p><h2 id="conditioning-combine" tabindex="-1"><a class="header-anchor" href="#conditioning-combine"><span>Conditioning (Combine)</span></a></h2><p><img src="'+I+'" alt="Conditioning (Combine) node"></p><p>条件化（合并），Conditioning (Combine) 节点可用于通过平均扩散模型的预测噪声来合并多个条件化。请注意，这与 Conditioning (Average) 节点不同。在这里，通过不同条件化（即构成条件化的所有部分）的扩散模型输出进行平均处理，而条件化（平均）节点则插值存储在条件化内部的文本嵌入。</p><p>!!! 提示 尽管 Conditioning Combine 没有一个因素输入来确定如何插值两个结果噪声预测，但可以使用 Conditioning (Set Area) 节点在组合它们之前对各个条件进行加权。</p><p><strong>输入</strong></p><p><code>conditioning_1</code></p><p>: 第一个条件。</p><p><code>conditioning_2</code></p><p>: 第二个条件。</p><p><strong>输出</strong></p><p><code>CONDITIONING</code></p><p>: 一个包含两个输入的新条件，稍后由采样器进行平均。</p><h2 id="conditioning-set-area" tabindex="-1"><a class="header-anchor" href="#conditioning-set-area"><span>Conditioning (Set Area)</span></a></h2><p><img src="'+b+'" alt="Conditioning (Set Area) node"></p><p>Conditioning (Set Area)节点可以用于将条件限制在图像的特定区域内。与Conditioning (Combine)节点一起使用，可以对最终图像的组合进行更多的控制。</p><p>!!! 提示 ComfyUI中坐标系统的原点位于左上角。在混合扩散模型的多个噪声预测之前，强度会被归一化。</p><p><strong>输入</strong></p><p><code>conditioning</code></p><p>: 将被限制在区域内的条件</p><p><code>width</code></p><p>: 区域的宽度</p><p><code>height</code></p><p>: 区域的高度</p><p><code>x</code></p><p>: 区域的x坐标</p><p><code>y</code></p><p>: 区域的y坐标</p><p><code>strength</code></p><p>: 当混合多个重叠的条件时使用的区域权重</p><p><strong>输出</strong></p><p><code>CONDITIONING</code></p><p>: 一个新的条件，限制在指定的区域内。</p><h3 id="conditioning-set-mask" tabindex="-1"><a class="header-anchor" href="#conditioning-set-mask"><span>Conditioning (Set Mask)</span></a></h3><p><img src="'+v+'" alt="Conditioning (Set Mask) node"></p><p>条件化（设置遮罩），Conditioning (Set Mask) 节点可用于将条件化限制在指定的遮罩中。与Conditioning (Combine) 节点一起使用，可以更好地控制最终图像的组合。</p><p>!!! 提示 强度在从扩散模型中混合多个噪声预测之前进行归一化。</p><p><strong>输入</strong></p><p><code>conditioning</code></p><p>: 限制到遮罩的条件化</p><p><code>mask</code></p><p>: 限制条件化的遮罩</p><p><code>strength</code></p><p>: 在混合多个重叠条件化时使用的遮罩区域的权重。</p><p><code>set_cond_area</code></p><p>: 是否对整个区域进行去噪，还是限制在遮罩的边界框内。</p><p><strong>输出</strong></p><p><code>CONDITIONING</code></p><p>: 一个新的条件化，限制在指定的遮罩中。</p><h2 id="example" tabindex="-1"><a class="header-anchor" href="#example"><span>example</span></a></h2><p>example usage text with workflow image</p><h3 id="gligen-textbox-apply" tabindex="-1"><a class="header-anchor" href="#gligen-textbox-apply"><span>GLIGEN Textbox Apply</span></a></h3><p><img src="'+w+'" alt="GLIGEN Textbox Apply node"></p><p>应用GLIGEN文本框，GLIGEN Textbox Apply节点可用于为扩散模型提供进一步的空间指导，引导其在图像的特定区域生成指定的部分。尽管文本输入可以接受任何文本，但GLIGEN最适合的输入是文本提示中的一部分对象。</p><p>!!! 提示 ComfyUI中的坐标系原点位于左上角。</p><p><strong>输入</strong></p><p><code>conditioning_to</code></p><p>: 一个条件.</p><p><code>clip</code></p><p>: CLIP模型.</p><p><code>gligen_textbox_model</code></p><p>: GLIGEN模型.</p><p><code>text</code></p><p>: 要与空间信息关联的文本</p><p><code>width</code></p><p>: 区域的宽度</p><p><code>height</code></p><p>: 区域的高度</p><p><code>x</code></p><p>: 区域的x坐标</p><p><code>y</code></p><p>: 区域的y坐标</p><p><strong>输出</strong></p><p><code>CONDITIONING</code></p><p>: 包含GLIGEN和空间指导的条件。</p><h2 id="example-1" tabindex="-1"><a class="header-anchor" href="#example-1"><span>example</span></a></h2><p>example usage text with workflow image</p><h1 id="unclip-conditioning" tabindex="-1"><a class="header-anchor" href="#unclip-conditioning"><span>unCLIP Conditioning</span></a></h1><p><img src="'+_+'" alt="unCLIP Conditioning node"></p><p>unCLIP条件化，unCLIP Conditioning 节点可以通过由CLIP视觉模型编码的图像为unCLIP模型提供额外的视觉指导。可以链接多个节点以提供多个图像作为指导。</p><p>!!! 提示 并非所有扩散模型都与unCLIP条件化兼容。此节点特别需要使用考虑到unCLIP的扩散模型。</p><p><strong>输入</strong></p><p><code>conditioning</code></p><p>: 条件化</p><p><code>clip_vision_output</code></p><p>: 由CLIP VISION模型编码的图像</p><p><code>strength</code></p><p>: unCLIP扩散模型应受图像指导的强度</p><p><code>noise_augmentation</code></p><p>: 用于将unCLIP扩散模型引导到原始CLIP视觉嵌入的随机位置，提供与编码图像密切相关的生成图像的额外变化</p><p><strong>输出</strong></p><p><code>CONDITIONING</code></p><p>: 包含unCLIP模型的额外视觉指导的条件化</p><h2 id="实验性-experimental" tabindex="-1"><a class="header-anchor" href="#实验性-experimental"><span>实验性 Experimental</span></a></h2><p>实验性包含实验性节点，可能尚未完全完善。</p><h3 id="load-latent" tabindex="-1"><a class="header-anchor" href="#load-latent"><span>Load Latent</span></a></h3><p><img src="'+C+'" alt="Load Latent node"></p><p>加载潜在节点：Load Latent节点可用于加载使用保存潜在节点保存的潜在图像。</p><p><strong>输入</strong></p><p><code>latent</code></p><p>: 潜在图像的名称</p><p><strong>输出</strong></p><p><code>LATENT</code></p><p>: 加载的潜在图像</p><h3 id="save-latent" tabindex="-1"><a class="header-anchor" href="#save-latent"><span>Save Latent</span></a></h3><p><img src="'+x+'" alt="Save Latent node"></p><p>保存潜变量节点：Save Latent节点可以用于保存潜变量以备将来使用。这些潜变量可以使用Load Latent节点再次加载。</p><p><strong>输入</strong></p><p><code>samples</code></p><p>: 要保存的潜变量</p><p><code>filename_prefix</code></p><p>: 文件名的前缀</p><p><strong>输出</strong></p><p>此节点没有输出</p><h3 id="tome-patch-model" tabindex="-1"><a class="header-anchor" href="#tome-patch-model"><span>Tome Patch Model</span></a></h3><p><img src="'+T+'" alt="Tome Patch Model node"></p><p>Tome Patch Model节点可以用于对扩散模型应用Tome优化。Tome（TOken MErging）试图找到一种合并提示令牌的方法，以使对最终图像的影响最小化。生成时间更快，所需的VRAM减少，但可能会降低质量。可以通过比率设置来控制此权衡，较高的值会导致合并更多的令牌。</p><p><strong>输入</strong></p><p><code>model</code></p><p>: 要应用Tome优化的扩散模型</p><p><code>ratio</code></p><p>: 确定何时合并令牌的阈值</p><p><strong>输出</strong></p><p><code>MODEL</code></p><p>: 经Tome优化的扩散模型</p><h2 id="tokenmerging-for-stable-diffusion" tabindex="-1"><a class="header-anchor" href="#tokenmerging-for-stable-diffusion"><span>TokenMerging for Stable Diffusion</span></a></h2><p><img src="'+y+'" alt="TokenMerging for Stable Diffusion"></p><hr><h3 id="vae-decode-tiled" tabindex="-1"><a class="header-anchor" href="#vae-decode-tiled"><span>VAE Decode (Tiled)</span></a></h3><p><img src="'+E+'" alt="VAE Decode (Tiled) node"></p><p>VAE解码（平铺），VAE Decode (Tiled) 节点可以使用提供的VAE将潜在空间图像解码回像素空间图像。该节点以平铺方式解码潜在图像，使其能够解码比常规VAE解码节点更大的潜在图像。</p><p>!!! 提示 当因为VRAM不足而导致常规VAE解码节点失败时，comfy将自动使用平铺实现进行重试。</p><p><strong>输入</strong></p><p><code>samples</code></p><p>: 要解码的潜在图像</p><p><code>vae</code></p><p>: 用于解码潜在图像的VAE</p><p><strong>输出</strong></p><p><code>IMAGE</code></p><p>: 解码后的图像。</p><h3 id="vae-encode-tiled" tabindex="-1"><a class="header-anchor" href="#vae-encode-tiled"><span>VAE Encode (Tiled)</span></a></h3><p><img src="'+N+'" alt="VAE Encode (Tiled) node"></p><p>VAE编码（平铺），VAE Encode (Tiled) 节点可用于使用提供的VAE将像素空间图像编码为潜在空间图像。此节点使用图块对图像进行编码，使其能够编码比常规VAE编码节点更大的图像。</p><p>!!! 提示 当常规VAE编码节点由于VRAM不足而失败时，Comfy会自动使用平铺实现进行重试。</p><p><strong>输入</strong></p><p><code>pixels</code></p><p>: 像素要编码的像素空间图像</p><p><code>vae</code></p><p>: 用于编码像素图像的VAE</p><p><strong>输出</strong></p><p><code>LATENT</code></p><p>: 编码的潜在图像。</p><h2 id="图像-image" tabindex="-1"><a class="header-anchor" href="#图像-image"><span>图像 Image</span></a></h2><p>ComfyUI提供了多种节点来操作像素图像。这些节点可以用于加载图像以进行图像转换工作流，保存结果，或者用于对图像进行高分辨率处理。</p><h3 id="load-image" tabindex="-1"><a class="header-anchor" href="#load-image"><span>Load Image</span></a></h3><p><img src="'+M+'" alt="Load Image node"></p><p>加载图像，Load Image节点可用于加载图像。可以通过启动文件对话框或将图像拖放到节点上来上传图像。一旦图像上传完成，就可以在节点内部选择它们。</p><p>!!! 提示 默认情况下，图像将上传到ComfyUI的输入文件夹中。</p><p><strong>输入</strong></p><p><code>image</code></p><p>: 图像要使用的名称。</p><p><strong>输出</strong></p><p><code>IMAGE</code></p><p>: 像素图像。</p><p><code>MASK</code></p><p>: 图像的Alpha通道。</p><h3 id="示例" tabindex="-1"><a class="header-anchor" href="#示例"><span>示例</span></a></h3><p>为了执行图像到图像的生成，您必须使用加载图像节点加载图像。在下面的示例中，使用加载图像节点加载了一张图像，然后使用 VAE encode 节点将其编码为潜在空间，从而使我们能够执行图像到图像的任务。</p><p>(TODO: provide different example using mask)</p><h3 id="invert-image" tabindex="-1"><a class="header-anchor" href="#invert-image"><span>Invert Image</span></a></h3><p><img src="'+V+'" alt="Invert Image node"></p><p>反转图像节点，Invert Image 节点可以用于反转图像的颜色。</p><p><strong>输入</strong></p><p><code>image</code></p><p>: 要反转的像素图像</p><p><strong>输出</strong></p><p><code>IMAGE</code></p><p>: 反转后的像素图像</p><h3 id="pad-image-for-outpainting" tabindex="-1"><a class="header-anchor" href="#pad-image-for-outpainting"><span>Pad Image for Outpainting</span></a></h3><p><img src="'+U+'" alt="Pad Image for Outpainting node"></p><p>Outpainting节点的Pad Image用于给图像添加填充，以进行outpainting。然后，可以通过VAE Encode for Inpainting将此图像输入到inpaint diffusion模型中。</p><p><strong>输入</strong></p><p><code>image</code></p><p>: 要进行填充的图像。</p><p><code>left</code></p><p>: 要在图像左侧填充的量。</p><p><code>top</code></p><p>: 要在图像上方填充的量。</p><p><code>right</code></p><p>: 要在图像右侧填充的量。</p><p><code>bottom</code></p><p>: 要在图像下方填充的量。</p><p><code>feathering</code></p><p>: 原始图像边界的羽化程度。</p><p><strong>输出</strong></p><p><code>IMAGE</code></p><p>: 填充后的像素图像。</p><p><code>MASK</code></p><p>: 指示采样器在哪里进行outpainting的掩码。</p><h3 id="preview-image" tabindex="-1"><a class="header-anchor" href="#preview-image"><span>Preview Image</span></a></h3><p><img src="'+A+'" alt="Preview Image node"> 预览图像节点可用于在节点图中预览图像。</p><p><strong>输入</strong></p><p><code>image</code></p><p>: 图像像素数据</p><p><strong>输出</strong></p><p>该节点没有输出参数</p><h3 id="save-image" tabindex="-1"><a class="header-anchor" href="#save-image"><span>Save Image</span></a></h3><p><img src="'+S+'" alt="Save Image node">{ align=right width=450 }</p>',235),Ne={href:"https://blenderneko.github.io/ComfyUI-docs/Interface/SaveFileFormatting/",target:"_blank",rel:"noopener noreferrer"},Me=n('<p><strong>输入</strong></p><p><code>image</code></p><p>: 要预览的像素图像</p><p><code>filename_prefix</code></p><p>: 要放入文件名中的前缀</p><p><strong>输出</strong></p><p>此节点没有输出。</p><h3 id="image-blend-postprocessing" tabindex="-1"><a class="header-anchor" href="#image-blend-postprocessing"><span>Image Blend#postprocessing</span></a></h3><p><img src="'+P+'" alt="Image Blend node"></p><p>The Image Blend node can be used to blend two images together.</p><p>!!! info If the dimensions of the second image do not match those of the first it is rescaled and center-cropped to maintain its aspect ratio</p><p><strong>输入</strong></p><p><code>image1</code></p><p>: A pixel image.</p><p><code>image2</code></p><p>: A second pixel image.</p><p><code>blend_factor</code></p><p>: The opacity of the second image.</p><p><code>blend_mode</code></p><p>: How to blend the images.</p><p><strong>输出</strong></p><p><code>IMAGE</code></p><p>: The blended pixel image.</p><h3 id="image-blur" tabindex="-1"><a class="header-anchor" href="#image-blur"><span>Image Blur</span></a></h3><p><img src="'+R+'" alt="Image Blur node"></p><p>The Image Blend node can be used to apply a gaussian blur to an image.</p><p><strong>输入</strong></p><p><code>image</code></p><p>: The pixel image to be blurred.</p><p><code>blur_radius</code></p><p>: The radius of the gaussian.</p><p><code>sigma</code></p><p>: The sigma of the gaussian, the smaller sigma is the more the kernel in concentrated on the center pixel.</p><p><strong>输出</strong></p><p><code>IMAGE</code></p><p>: The blurred pixel image.</p><h2 id="image-quantize" tabindex="-1"><a class="header-anchor" href="#image-quantize"><span>Image Quantize</span></a></h2><p><img src="'+F+'" alt="Image Quantize node"></p><p>The Image Quantize node can be used to quantize an image, reducing the number of colors in the image.</p><p><strong>输入</strong></p><p><code>image</code></p><p>: The pixel image to be quantized.</p><p><code>colors</code></p><p>: The number of colors in the quantized image.</p><p><code>dither</code></p><p>: Wether to use dithering to make the quantized image look more smooth, or not.</p><p><strong>输出</strong></p><p><code>IMAGE</code></p><p>: The quantized pixel image.</p><h2 id="image-sharpen" tabindex="-1"><a class="header-anchor" href="#image-sharpen"><span>Image Sharpen</span></a></h2><p><img src="'+Z+'" alt="Image Sharpen node"></p><p>The Image Sharpen node can be used to apply a Laplacian sharpening filter to an image.</p><p><strong>输入</strong></p><p><code>image</code></p><p>: The pixel image to be sharpened.</p><p><code>sharpen_radius</code></p><p>: The radius of the sharpening kernel.</p><p><code>sigma</code></p><p>: The sigma of the gaussian, the smaller sigma is the more the kernel in concentrated on the center pixel.</p><p><code>alpha</code></p><p>: The strength of the sharpening kernel.</p><p><strong>输出</strong></p><p><code>IMAGE</code></p><p>: The sharpened pixel image.</p><h2 id="upscale-image" tabindex="-1"><a class="header-anchor" href="#upscale-image"><span>Upscale Image</span></a></h2><p><img src="'+Q+'" alt="Upscale Image node"></p>',66),Ve=n('<p><strong>输入</strong></p><p><code>image</code></p><p>: The pixel images to be upscaled.</p><p><code>upscale_method</code></p><p>: The method used for resizing.</p><p><code>Width</code></p><p>: The target width in pixels.</p><p><code>height</code></p><p>: The target height in pixels.</p><p><code>crop</code></p><p>: Wether or not to center-crop the image to maintain the aspect ratio of the original latent images.</p><p><strong>输出</strong></p><p><code>IMAGE</code></p><p>: The resized images.</p><h2 id="upscale-image-using-model" tabindex="-1"><a class="header-anchor" href="#upscale-image-using-model"><span>Upscale Image (using Model)</span></a></h2><p><img src="'+B+'" alt="Upscale Image (using Model) node"></p><p>The Upscale Image (using Model) node can be used to upscale pixel images using a model loaded with the [Load Upscale Model]</p><p><strong>输入</strong></p><p><code>upscale_model</code></p><p>: The model used for upscaling.</p><p><code>image</code></p><p>: The pixel images to be upscaled.</p><p><strong>输出</strong></p><p><code>IMAGE</code></p><p>: The upscaled images.</p><h2 id="latent-潜在空间" tabindex="-1"><a class="header-anchor" href="#latent-潜在空间"><span>Latent 潜在空间</span></a></h2><p>潜在扩散模型（如稳定扩散）不在像素空间中运作，而是在潜空间中进行去噪。这些节点提供了使用编码器和解码器在像素空间和潜在空间之间切换的方法，并提供了多种操纵潜在图像的方式。</p><h3 id="empty-latent-image" tabindex="-1"><a class="header-anchor" href="#empty-latent-image"><span>Empty Latent Image</span></a></h3><p><img src="'+D+'" alt="Empty Latent Image node"></p><p>空潜像图节点，Empty Latent Image节点可用于创建一组新的空潜像图。这些潜像图可以在文本转图像工作流中使用，通过采样器节点对其进行添加噪声和去噪处理。</p><p><strong>输入</strong></p><p><code>width</code></p><p>: 宽度（像素）</p><p><code>height</code></p><p>: 高度（像素）；</p><p><code>batch_size</code></p><p>: 批次大小（潜在图像数量）。</p><p><strong>输出</strong></p><p><code>LATENT</code></p><p>: 空潜在图像</p><h3 id="latent-composite" tabindex="-1"><a class="header-anchor" href="#latent-composite"><span>Latent Composite</span></a></h3><p><img src="'+W+'" alt="Latent Composite node"></p><p>潜在图像合成 Latent Composite节点可用于将一个潜在图像合成到另一个潜在图像中。</p><p>!!! 提示 ComfyUI中的坐标系统原点位于左上角。</p><p><strong>输入</strong></p><p><code>samples_to</code></p><p>: 要复合的潜在图像。</p><p><code>samples_from</code></p><p>: 要粘贴的潜在图像。</p><p><code>x</code></p><p>: 粘贴潜在图像的x坐标（以像素为单位）。</p><p><code>y</code></p><p>: 粘贴潜在图像的y坐标（以像素为单位）。</p><p><code>feather</code></p><p>: 要粘贴的潜在图像的羽化效果。</p><p><strong>输出</strong></p><p><code>LATENT</code></p><p>: 包含将 samples_from 粘贴到 samples_to 中的新潜在图像组合。</p><h3 id="latent-composite-masked" tabindex="-1"><a class="header-anchor" href="#latent-composite-masked"><span>Latent Composite Masked</span></a></h3><p><img src="'+G+'" alt="Latent Composite Masked node"></p><p>潜在图像合成遮罩,Latent Composite Masked节点可用于将一个潜在图像遮罩复合体粘贴到另一个潜在图像。</p><p><strong>输入</strong></p><p><code>destination</code></p><p>: 目的地：要粘贴的潜在图像。</p><p><code>source</code></p><p>: 源：要粘贴的潜在图像。</p><p><code>mask</code></p><p>: 遮罩：要粘贴的源潜在图像的遮罩。</p><p><code>x</code></p><p>: 粘贴潜在图像的x坐标（以像素为单位）</p><p><code>y</code></p><p>: 粘贴潜在图像的y坐标（以像素为单位）</p><p><strong>输出</strong></p><p><code>LATENT</code></p><p>: 将源潜在图像粘贴到目标潜像的组合而成的新潜在图像。</p><h3 id="upscale-latent" tabindex="-1"><a class="header-anchor" href="#upscale-latent"><span>Upscale Latent</span></a></h3><p><img src="'+z+'" alt="Upscale Latent node"></p><p>放大潜在图像，Upscale Latent 节点可用于调整潜在图像的大小。</p><p>!!! 提示： 调整潜像图的大小与调整像素图像的大小不同。简单地调整潜像图而不是像素会导致更多的伪影。</p><p><strong>输入</strong></p><p><code>samples</code></p><p>: 要调整大小的潜像图</p><p><code>upscale_method</code></p><p>: 用于调整大小的方法。</p><p><code>Width</code></p><p>: 目标宽度（以像素为单位）。</p><p><code>height</code></p><p>: 目标高度（以像素为单位）。</p><p><code>crop</code></p><p>: 是否居中裁剪图像以保持原始潜像图的纵横比。</p><p><strong>输出</strong></p><p><code>LATENT</code></p><p>: 调整大小后的潜像图。</p><h3 id="vae-decode" tabindex="-1"><a class="header-anchor" href="#vae-decode"><span>VAE Decode</span></a></h3><p><img src="'+O+'" alt="VAE Decode node"></p><p>VAE解码，VAE Decode节点可用于使用提供的VAE将潜像图解码为像素图像。</p><p><strong>输入</strong></p><p><code>samples</code></p><p>: 要解码的潜像图。</p><p><code>vae</code></p><p>: 用于解码潜像图的VAE。</p><p><strong>输出</strong></p><p><code>IMAGE</code></p><p>: 解码后的像素图像。</p><h2 id="example-2" tabindex="-1"><a class="header-anchor" href="#example-2"><span>example</span></a></h2><p>TODO: SD 1.5 to XL example</p><h3 id="vae-encode" tabindex="-1"><a class="header-anchor" href="#vae-encode"><span>VAE Encode</span></a></h3><p><img src="'+j+'" alt="VAE Encode node"></p><p>VAE编码，VAE Encode 节点可以使用提供的VAE将像素空间图像编码为潜像图。</p><p><strong>输入</strong></p><p><code>pixels</code></p><p>: 像素：要编码的像素空间图像。</p><p><code>vae</code></p><p>: 用于编码像素图像的VAE。</p><p><strong>输出</strong></p><p><code>LATENT</code></p><p>: 编码的潜像图。</p><h2 id="示例-1" tabindex="-1"><a class="header-anchor" href="#示例-1"><span>示例</span></a></h2><p>为了在图像到图像的任务中使用图像，首先需要将其编码为潜像图。在下面的示例中，使用VAE编码节点将像素图像转换为潜像图，以便我们可以将其重新噪声化和去噪，得到全新的图像。</p><h3 id="latent-from-batch" tabindex="-1"><a class="header-anchor" href="#latent-from-batch"><span>Latent From Batch</span></a></h3><p><img src="'+H+'" alt="Latent From Batch node"></p><p>从批次中提取潜像图，Latent From Batch 节点可以用于从批次中选择一个潜像图或图像片段。这在工作流中需要隔离特定的潜像图或图像时非常有用。</p><p><strong>输入</strong></p><p><code>samples</code></p><p>: 要选择一个片段的批次潜像图。</p><p><code>batch_index</code></p><p>: 要选择的第一个潜像图的索引。</p><p><code>length</code></p><p>: 要获取的潜像图数量。</p><p><strong>输出</strong></p><p><code>LATENT</code></p><p>: 只包含所选择片段的新批次潜像图</p><h3 id="rebatch-latents" tabindex="-1"><a class="header-anchor" href="#rebatch-latents"><span>Rebatch Latents</span></a></h3><p><img src="'+J+'" alt="Rebatch Latents node"></p><p>重新分批潜像图，Rebatch Latents节点可以用于拆分或合并批量的潜在空间图像。当这导致多个批次时，该节点将输出一个批次列表，而不是单个批次。这在批量大小过大无法全部适应VRAM内时非常有用，因为ComfyUI将对列表中的每个批次执行节点，而不是一次执行全部。它还可以将批次列表合并回一个单独的批次中。</p><p><strong>输入</strong></p><p><code>samples</code></p><p>: 待重新分批的潜图。</p><p><code>batch_size</code></p><p>: 新的批次大小。</p><p><strong>输出</strong></p><p><code>LATENT</code></p><p>: 一个潜图列表，其中每个批次的大小不超过batch_size。</p><h3 id="repeat-latent-batch" tabindex="-1"><a class="header-anchor" href="#repeat-latent-batch"><span>Repeat Latent Batch</span></a></h3><p><img src="'+Y+'" alt="Repeat Latent Batch node"></p><p>重复潜在批处理，Repeat Latent Batch节点可用于重复一批潜像图。这可以用于在图像到图像工作流中创建多个图像变体。</p><p><strong>输入</strong></p><p><code>samples</code></p><p>: 要重复的潜像图批处理。</p><p><code>amount</code></p><p>: 重复的次数。</p><p><strong>输出</strong></p><p><code>LATENT</code></p><p>: 重复了指定次数的新的潜像图批处理。</p><h3 id="set-latent-noise-mask" tabindex="-1"><a class="header-anchor" href="#set-latent-noise-mask"><span>Set Latent Noise Mask</span></a></h3><p><img src="'+X+'" alt="Set Latent Noise Mask node"></p><p>设置潜在噪声掩码，Set Latent Noise Mask节点可用于为修复图像的潜像图添加掩码。当设置了噪声掩码时，采样节点将仅在掩码区域上操作。如果提供了单个掩码，批处理中的所有潜像图都将使用该掩码。</p><p><strong>输入</strong></p><p><code>samples</code></p><p>: 样本：要修复的潜像图。</p><p><code>mask</code></p><p>: 掩码：指示修复位置的掩码。</p><p><strong>输出</strong></p><p><code>LATENT</code></p><p>: 潜在：经过掩码处理的潜像图。</p><h3 id="vae-encode-for-inpainting" tabindex="-1"><a class="header-anchor" href="#vae-encode-for-inpainting"><span>VAE Encode (for Inpainting)</span></a></h3><p><img src="'+q+'" alt="VAE Encode For Inpainting node"></p><p>VAE编码（用于修复图像），VAE Encode (for Inpainting) 将像素空间图像编码为潜在空间图像，使用提供的VAE（变分自编码器）。它还接受修复图像的掩码，指示采样节点应该对图像的哪些部分去噪。可以使用grow_mask_by来增加掩码区域的大小，为修复过程提供一些额外的填充区域。</p><p>!!! 提示 该节点专门用于用于修复训练的扩散模型，并确保在编码之前，掩码下方的像素被设置为灰色（0.5,0.5,0.5）。</p><p><strong>输入</strong></p><p><code>pixels</code></p><p>: 像素：要编码的像素空间图像。</p><p><code>vae</code></p><p>: 用于编码像素图像的VAE。</p><p><code>mask</code></p><p>: 掩码：指示要修复的位置的掩码。</p><p><code>grow_mask_by</code></p><p>: 增加给定掩码区域的大小。</p><p><strong>输出</strong></p><p><code>LATENT</code></p><p>: 掩码和编码的潜像图。</p><h3 id="crop-latent" tabindex="-1"><a class="header-anchor" href="#crop-latent"><span>Crop Latent</span></a></h3><p><img src="'+K+'" alt="Crop Latent node"></p><p>裁剪潜像，Crop latent 节点可用于将潜像裁剪到新的形状。</p><p><strong>输入</strong></p><p><code>samples</code></p><p>: 样本要裁剪的潜像。</p><p><code>width</code></p><p>: 宽度以像素为单位的区域宽度。</p><p><code>height</code></p><p>: 高度以像素为单位的区域高度。</p><p><code>x</code></p><p>: 以像素为单位的区域x坐标。</p><p><code>y</code></p><p>: 以像素为单位的区域y坐标。</p><p><strong>输出</strong></p><p><code>LATENT</code></p><p>: 裁剪后的潜像图。</p><h3 id="flip-latent" tabindex="-1"><a class="header-anchor" href="#flip-latent"><span>Flip Latent</span></a></h3><p><img src="'+$+'" alt="Flip Latent node"></p><p>翻转潜像，Flip Latent 节点可用于水平或垂直翻转潜像。</p><p><strong>输入</strong></p><p><code>samples</code></p><p>: 要翻转的潜像。</p><p><code>flip_method</code></p><p>: 选择水平翻转或垂直翻转</p><p><strong>输出</strong></p><p><code>LATENT</code></p><p>: 翻转后的潜像图。</p><h3 id="rotate-latent" tabindex="-1"><a class="header-anchor" href="#rotate-latent"><span>Rotate Latent</span></a></h3><p><img src="'+ee+'" alt="Rotate Latent node"></p><p>The Rotate Latent node can be used to rotate latent images clockwise in increments of 90 degrees.</p><p><strong>输入</strong></p><p><code>samples</code></p><p>: The latent images to be rotated.</p><p><code>rotation</code></p><p>: Clockwise rotation.</p><p><strong>输出</strong></p><p><code>LATENT</code></p><p>: The rotated latents.</p><h2 id="载入器-loaders" tabindex="-1"><a class="header-anchor" href="#载入器-loaders"><span>载入器 Loaders</span></a></h2><p>The loaders in this segment can be used to load a variety of models used in various workflows. A full list of all of the loaders can be found in the sidebar.</p><h3 id="gligen-loader" tabindex="-1"><a class="header-anchor" href="#gligen-loader"><span>GLIGEN Loader</span></a></h3><p><img src="'+oe+'" alt="GLIGEN Loader node"></p><p>The GLIGEN Loader node can be used to load a specific GLIGEN model. GLIGEN models are used to associate spatial information to parts of a text prompt, guiding the diffusion model to generate images adhering to compositions specified by GLIGEN.</p><p><strong>输入</strong></p><p><code>gligen_name</code></p><p>: The name of the GLIGEN model.</p><p><strong>输出</strong></p><p><code>GLIGEN</code></p><p>: The GLIGEN model used to encode spatial information to parts of the text prompt.</p><h3 id="hypernetwork-loader" tabindex="-1"><a class="header-anchor" href="#hypernetwork-loader"><span>Hypernetwork Loader</span></a></h3><p><img src="'+pe+`" alt="Hypernetwork Loader node"></p><p>The Hypernetwork Loader node can be used to load a hypernetwork. similar to LoRAs, they are used to modify the diffusion model, to alter the way in which latents are denoised. Typical use-cases include adding to the model the ability to generate in certain styles, or better generate certain subjects or actions. One can even chain multiple hypernetworks together to further modify the model.</p><p>!!! tip</p><pre><code>Hypernetwork strength values can be set to negative values. At times this can result in interesting effects.
</code></pre><h2 id="inputs" tabindex="-1"><a class="header-anchor" href="#inputs"><span>inputs</span></a></h2><p><code>model</code></p><p>: A diffusion model.</p><p><code>hypernetwork_name</code></p><p>: The name of the hypernetwork.</p><p><code>strength</code></p><p>: How strongly to modify the diffusion model. This value can be negative.</p><h2 id="outputs" tabindex="-1"><a class="header-anchor" href="#outputs"><span>outputs</span></a></h2><p><code>MODEL</code></p><p>: The modified diffusion model.</p><h3 id="load-checkpoint" tabindex="-1"><a class="header-anchor" href="#load-checkpoint"><span>Load Checkpoint</span></a></h3><p><img src="`+te+'" alt="Load Checkpoint node"></p><p>The Load Checkpoint node can be used to load a diffusion model, diffusion models are used to denoise latents. This node will also provide the appropriate VAE and CLIP model.</p><h2 id="inputs-1" tabindex="-1"><a class="header-anchor" href="#inputs-1"><span>inputs</span></a></h2><p><code>ckpt_name</code></p><p>: The name of the model.</p><h2 id="outputs-1" tabindex="-1"><a class="header-anchor" href="#outputs-1"><span>outputs</span></a></h2><p><code>MODEL</code></p><p>: The model used for denoising latents.</p><p><code>CLIP</code></p><p>: The CLIP model used for encoding text prompts.</p><p><code>VAE</code></p><p>: The VAE model used for encoding and decoding images to and from latent space.</p><h3 id="load-clip" tabindex="-1"><a class="header-anchor" href="#load-clip"><span>Load CLIP</span></a></h3><p><img src="'+ae+`" alt="Load CLIP node"></p><p>The Load CLIP node can be used to load a specific CLIP model, CLIP models are used to encode text prompts that guide the diffusion process.</p><p>!!! warning</p><pre><code>Conditional diffusion models are trained using a specific CLIP model, using a different model than the one which it was trained with is unlikely to result in good images. The [Load Checkpoint](LoadCheckpoint.md) node automatically loads the correct CLIP model.
</code></pre><h2 id="inputs-2" tabindex="-1"><a class="header-anchor" href="#inputs-2"><span>inputs</span></a></h2><p><code>clip_name</code></p><p>: The name of the CLIP model.</p><h2 id="outputs-2" tabindex="-1"><a class="header-anchor" href="#outputs-2"><span>outputs</span></a></h2><p><code>CLIP</code></p><p>: The CLIP model used for encoding text prompts.</p><h3 id="load-clip-vision" tabindex="-1"><a class="header-anchor" href="#load-clip-vision"><span>Load CLIP Vision</span></a></h3><p><img src="`+ne+'" alt="Load CLIP Vision node"></p><p>The Load CLIP Vision node can be used to load a specific CLIP vision model, similar to how CLIP models are used to encode text prompts, CLIP vision models are used to encode images.</p><h2 id="inputs-3" tabindex="-1"><a class="header-anchor" href="#inputs-3"><span>inputs</span></a></h2><p><code>clip_name</code></p><p>: The name of the CLIP vision model.</p><h2 id="outputs-3" tabindex="-1"><a class="header-anchor" href="#outputs-3"><span>outputs</span></a></h2><p><code>CLIP_VISION</code></p><p>: The CLIP vision model used for encoding image prompts.</p><h3 id="load-controlnet-model" tabindex="-1"><a class="header-anchor" href="#load-controlnet-model"><span>Load ControlNet Model</span></a></h3><p><img src="'+se+'" alt="Load ControlNet node"></p><p>The Load ControlNet Model node can be used to load a ControlNet model. Similar to how the CLIP model provides a way to give textual hints to guide a diffusion model, ControlNet models are used to give visual hints to a diffusion model. This process is different from e.g. giving a diffusion model a partially noised up image to modify. Instead ControlNet models can be used to tell the diffusion model e.g. where edges in the final image should be, or how subjects should be posed. This node can also be used to load T2IAdaptors.</p><h2 id="inputs-4" tabindex="-1"><a class="header-anchor" href="#inputs-4"><span>inputs</span></a></h2><p><code>control_net_name</code></p><p>: The name of the ControlNet model.</p><h2 id="outputs-4" tabindex="-1"><a class="header-anchor" href="#outputs-4"><span>outputs</span></a></h2><p><code>CONTROL_NET</code></p><p>: The ControlNet or T2IAdaptor model used for providing visual hints to a diffusion model.</p><h3 id="load-lora" tabindex="-1"><a class="header-anchor" href="#load-lora"><span>Load LoRA</span></a></h3><p><img src="'+le+`" alt="Load LoRA node"></p><p>The Load LoRA node can be used to load a LoRA. LoRAs are used to modify the diffusion and CLIP models, to alter the way in which latents are denoised. Typical use-cases include adding to the model the ability to generate in certain styles, or better generate certain subjects or actions. One can even chain multiple LoRAs together to further modify the model.</p><p>!!! tip</p><pre><code>LoRA strength values can be set to negative values. At times this can result in interesting effects.
</code></pre><h2 id="inputs-5" tabindex="-1"><a class="header-anchor" href="#inputs-5"><span>inputs</span></a></h2><p><code>model</code></p><p>: A diffusion model.</p><p><code>clip</code></p><p>: A CLIP model.</p><p><code>lora_name</code></p><p>: The name of the LoRA.</p><p><code>strength_model</code></p><p>: How strongly to modify the diffusion model. This value can be negative.</p><p><code>strength_clip</code></p><p>: How strongly to modify the CLIP model. This value can be negative.</p><h2 id="outputs-5" tabindex="-1"><a class="header-anchor" href="#outputs-5"><span>outputs</span></a></h2><p><code>MODEL</code></p><p>: The modified diffusion model.</p><p><code>CLIP</code></p><p>: The modified CLIP model.</p><h3 id="load-style-model" tabindex="-1"><a class="header-anchor" href="#load-style-model"><span>Load Style Model</span></a></h3><p><img src="`+de+'" alt="Load Style Model node"></p><p>The Load Style Model node can be used to load a Style model. Style models can be used to provide a diffusion model a visual hint as to what kind of style the denoised latent should be in.</p><p>!!! info Only T2IAdaptor style models are currently supported</p><h2 id="inputs-6" tabindex="-1"><a class="header-anchor" href="#inputs-6"><span>inputs</span></a></h2><p><code>style_model_name</code></p><p>: The name of the style model.</p><h2 id="outputs-6" tabindex="-1"><a class="header-anchor" href="#outputs-6"><span>outputs</span></a></h2><p><code>STYLE_MODEL</code></p><p>: The style model used for providing visual hints about the desired style to a diffusion model.</p><h3 id="load-upscale-model" tabindex="-1"><a class="header-anchor" href="#load-upscale-model"><span>Load Upscale Model</span></a></h3><p><img src="'+ie+'" alt="Load Upscale Model node"></p><p>The Load Upscale Model node can be used to load a specific upscale model, upscale models are used to upscale images.</p><h2 id="inputs-7" tabindex="-1"><a class="header-anchor" href="#inputs-7"><span>inputs</span></a></h2><p><code>model_name</code></p><p>: The name of the upscale model.</p><h2 id="outputs-7" tabindex="-1"><a class="header-anchor" href="#outputs-7"><span>outputs</span></a></h2><p><code>UPSCALE_MODEL</code></p><p>: The upscale model used for upscaling images.</p><h3 id="load-vae" tabindex="-1"><a class="header-anchor" href="#load-vae"><span>Load VAE</span></a></h3><p><img src="'+ce+'" alt="Load VAE node"></p>',330),Ue=n('<h2 id="inputs-8" tabindex="-1"><a class="header-anchor" href="#inputs-8"><span>inputs</span></a></h2><p><code>vae_name</code></p><p>: The name of the VAE.</p><h2 id="outputs-8" tabindex="-1"><a class="header-anchor" href="#outputs-8"><span>outputs</span></a></h2><p><code>VAE</code></p><p>: The VAE model used for encoding and decoding images to and from latent space.</p><h2 id="example-3" tabindex="-1"><a class="header-anchor" href="#example-3"><span>example</span></a></h2>',7),Ae=n('<h3 id="unclip-checkpoint-loader" tabindex="-1"><a class="header-anchor" href="#unclip-checkpoint-loader"><span>unCLIP Checkpoint Loader</span></a></h3><p><img src="'+re+'" alt="unCLIP Checkpoint Loader node"></p><p>The unCLIP Checkpoint Loader node can be used to load a diffusion model specifically made to work with unCLIP. unCLIP Diffusion models are used to denoise latents conditioned not only on the provided text prompt, but also on provided images. This node will also provide the appropriate VAE and CLIP amd CLIP vision models.</p><p>!!! warning even though this node can be used to load all diffusion models, not all diffusion models are compatible with unCLIP.</p><h2 id="inputs-9" tabindex="-1"><a class="header-anchor" href="#inputs-9"><span>inputs</span></a></h2><p><code>ckpt_name</code></p><p>: The name of the model.</p><h2 id="outputs-9" tabindex="-1"><a class="header-anchor" href="#outputs-9"><span>outputs</span></a></h2><p><code>MODEL</code></p><p>: The model used for denoising latents.</p><p><code>CLIP</code></p><p>: The CLIP model used for encoding text prompts.</p><p><code>VAE</code></p><p>: The VAE model used for encoding and decoding images to and from latent space.</p><p><code>CLIP_VISION</code></p><p>: The CLIP Vision model used for encoding image prompts.</p><h2 id="遮罩-mask" tabindex="-1"><a class="header-anchor" href="#遮罩-mask"><span>遮罩 Mask</span></a></h2><p>Masks provide a way to tell the sampler what to denoise and what to leave alone. These nodes provide a variety of ways create or load masks and manipulate them.</p><h3 id="将图像转换为遮罩" tabindex="-1"><a class="header-anchor" href="#将图像转换为遮罩"><span>将图像转换为遮罩</span></a></h3><p><img src="'+he+'" alt="Convert Image to Mask node"></p><p>将图像转换为遮罩节点允许用户从图像的特定通道创建遮罩。这对于想要控制图像特定区域的编辑或处理特别有用，例如只对图像的某一部分应用效果或调整。</p><p><strong>输入</strong></p><p><code>image</code></p><p>: 要转换为遮罩的像素图像。</p><p><code>channel</code></p><p>: 要用作遮罩的通道。</p><p><strong>输出</strong></p><p><code>MASK</code></p><p>: 由图像通道创建的遮罩。</p><h2 id="示例-2" tabindex="-1"><a class="header-anchor" href="#示例-2"><span>示例</span></a></h2><p>将图像转换为遮罩的操作非常直接。首先，你需要准备一个图像，并确定你想要用作遮罩的特定通道。以下是一个简单的工作流程示例：</p><ol><li>将你想要转换的图像连接到<code>image</code>输入。</li><li>选择你想要用作遮罩的通道（红色、绿色、蓝色或透明度）。</li><li>节点将处理输入的图像，并在<code>MASK</code>输出中提供相应通道的遮罩。</li></ol><p>这在许多图像处理任务中都非常有用，特别是在需要精确控制应用于图像哪个部分的效果时。</p><p>这样，你就可以利用图像的不同通道来创建精确的遮罩，进一步提高你的图像编辑能力和精度。</p><h3 id="裁剪遮罩" tabindex="-1"><a class="header-anchor" href="#裁剪遮罩"><span>裁剪遮罩</span></a></h3><p><img src="'+me+'" alt="Crop Mask node"></p><p>裁剪遮罩节点用于将遮罩裁剪为新的形状。通过定义裁剪区域的尺寸和坐标，你可以精确控制保留遮罩的哪一部分。</p><p>!!! info 在ComfyUI中，坐标系的原点位于左上角。</p><h2 id="输入" tabindex="-1"><a class="header-anchor" href="#输入"><span>输入</span></a></h2><p><code>mask</code></p><p>: 要裁剪的遮罩。</p><p><code>width</code></p><p>: 裁剪区域的宽度，以像素为单位。</p><p><code>height</code></p><p>: 裁剪区域的高度，以像素为单位。</p><p><code>x</code></p><p>: 裁剪区域的x坐标，以像素为单位。</p><p><code>y</code></p><p>: 裁剪区域的y坐标，以像素为单位。</p><h2 id="输出" tabindex="-1"><a class="header-anchor" href="#输出"><span>输出</span></a></h2><p><code>MASK</code></p><p>: 被裁剪后的遮罩。</p><h2 id="示例-3" tabindex="-1"><a class="header-anchor" href="#示例-3"><span>示例</span></a></h2><p>裁剪遮罩是图像编辑中常用的一项功能，它可以帮助你从一个大的遮罩中精确地选出你需要的部分。以下是如何使用裁剪遮罩节点的一个简单示例：</p><ol><li>将你想要裁剪的遮罩连接到<code>mask</code>输入。</li><li>设置<code>width</code>和<code>height</code>来确定裁剪区域的尺寸。</li><li>通过<code>x</code>和<code>y</code>输入来定位裁剪区域的左上角起始点。</li><li>节点将输出一个新的、按照你指定参数裁剪过的遮罩。</li></ol><p>此裁剪遮罩可用于去除不需要的部分，或者将焦点集中在遮罩的特定区域。</p><h3 id="羽化遮罩" tabindex="-1"><a class="header-anchor" href="#羽化遮罩"><span>羽化遮罩</span></a></h3><p><img src="'+ge+'" alt="Feather Mask node"></p><p>羽化遮罩节点用于对遮罩进行羽化处理，使遮罩边缘更加柔和，以实现更自然的过渡。</p><h2 id="输入-1" tabindex="-1"><a class="header-anchor" href="#输入-1"><span>输入</span></a></h2><p><code>mask</code></p><p>: 需要进行羽化处理的遮罩。</p><p><code>left</code></p><p>: 左侧边缘的羽化量。</p><p><code>top</code></p><p>: 顶部边缘的羽化量。</p><p><code>right</code></p><p>: 右侧边缘的羽化量。</p><p><code>bottom</code></p><p>: 底部边缘的羽化量。</p><h2 id="输出-1" tabindex="-1"><a class="header-anchor" href="#输出-1"><span>输出</span></a></h2><p><code>MASK</code></p><p>: 经过羽化处理的遮罩。</p><h2 id="示例-4" tabindex="-1"><a class="header-anchor" href="#示例-4"><span>示例</span></a></h2><p>羽化遮罩是在图像处理和合成中实现柔和边缘效果的重要工具，特别是当将两个图像层叠加或融合时，羽化可以使边缘之间的过渡更加自然。以下是一个简单的使用案例：</p><ol><li>将您想要羽化的遮罩连接到<code>mask</code>输入。</li><li>根据需要设置<code>left</code>、<code>top</code>、<code>right</code>和<code>bottom</code>，这些值确定了各个方向上羽化效果的强度。</li><li>节点会生成一个新的羽化遮罩，可以被进一步用在图像融合或其他视觉效果中。</li></ol><p>此羽化遮罩可用于平滑遮罩的硬边缘，避免图像合成时出现不自然的硬线条或过渡。</p><h3 id="反转遮罩" tabindex="-1"><a class="header-anchor" href="#反转遮罩"><span>反转遮罩</span></a></h3><p><img src="'+ue+'" alt="Invert Mask node"></p><p>反转遮罩节点用于反转一个遮罩，将原本的掩盖区域转换为非掩盖区域，反之亦然。这在您希望改变遮罩影响区域时非常有用。</p><h2 id="输入-2" tabindex="-1"><a class="header-anchor" href="#输入-2"><span>输入</span></a></h2><p><code>mask</code></p><p>: 需要被反转的遮罩。</p><h2 id="输出-2" tabindex="-1"><a class="header-anchor" href="#输出-2"><span>输出</span></a></h2><p><code>MASK</code></p><p>: 被反转后的遮罩。</p><h2 id="实例" tabindex="-1"><a class="header-anchor" href="#实例"><span>实例</span></a></h2><p>反转遮罩节点在各种情境下都非常有用，尤其是当您希望对图像的不同部分应用不同的效果或处理时。以下是一个简单的使用案例：</p><p>在这个工作流中，我们首先加载一个遮罩，该遮罩原本是为了保护图像的一个区域而创建的。但在某一步骤，我们决定想对原本被保护的区域应用某种效果（比如去噪或滤镜），同时保持原本未被保护的区域不变。这时，我们就可以使用反转遮罩节点。</p><ol><li>加载您原本的遮罩到工作流。</li><li>将遮罩连接到“反转遮罩”节点的输入端。</li><li>将“反转遮罩”节点的输出连接到您的图像处理节点（比如 KSampler）。</li></ol><p>现在，原本被保护的区域将接受处理，而其他区域则保持不变。</p><p>通过使用反转遮罩节点，您能轻松地在不同的图像编辑阶段切换受影响的区域，无需创建和加载多个遮罩文件。</p><h3 id="加载图片-作为遮罩" tabindex="-1"><a class="header-anchor" href="#加载图片-作为遮罩"><span>加载图片（作为遮罩）</span></a></h3><p><img src="'+fe+'" alt="Load Image (as Mask) node"></p><p>加载图片（作为遮罩）节点可以用来加载图片的某个通道并将其作为遮罩使用。图片可以通过启动文件对话框上传，或者直接拖拽到节点上。一旦图片上传，就可以在节点内部选择使用。</p><p>!!! info 默认情况下，图片会被上传到ComfyUI的输入文件夹</p><h2 id="输入-3" tabindex="-1"><a class="header-anchor" href="#输入-3"><span>输入</span></a></h2><p><code>image</code></p><p>: 要转换为遮罩的图片的名称。</p><p><code>channel</code></p><p>: 要用作遮罩的图片的通道。</p><h2 id="输出-3" tabindex="-1"><a class="header-anchor" href="#输出-3"><span>输出</span></a></h2><p><code>MASK</code></p><p>: 从图片通道创建的遮罩。</p><h2 id="实例-1" tabindex="-1"><a class="header-anchor" href="#实例-1"><span>实例</span></a></h2><p>加载图片作为遮罩的节点在您需要基于现有图像创建遮罩时非常有用，尤其是当图像的某个区域有独特的颜色或亮度时，可以通过该颜色或亮度区分进行遮罩。以下是一个简单的使用案例：</p><p>在此工作流中，我们有一个图像，其中某个区域我们想要进行特殊处理（例如应用特效、调整或变换），而不影响图像的其他部分。</p><ol><li>使用“加载图片（作为遮罩）”节点，选择您的图片文件。</li><li>在“通道”选项中，选择一个特定的颜色通道（红色、绿色、蓝色或透明度），这取决于您的图片和需要遮罩的区域。例如，如果需要遮罩的区域在红色通道上有高亮度，则选择红色通道。</li><li>将此节点的输出连接到下一个处理节点的遮罩输入，例如，一个应用特效的节点。</li></ol><p>现在，您的特效或处理将只应用于基于所选图片通道生成的遮罩的区域。</p><p>这种方法提供了一种简便的方式，通过现有图片快速创建遮罩，无需手动绘制复杂的遮罩形状。</p><h3 id="遮罩合成" tabindex="-1"><a class="header-anchor" href="#遮罩合成"><span>遮罩合成</span></a></h3><p><img src="'+ke+'" alt="Mask Composite node"></p><p>遮罩合成节点可以用来将一个遮罩粘贴到另一个遮罩中。</p><p>!!! info 在ComfyUI中，坐标系统的原点位于左上角。</p><h2 id="输入-4" tabindex="-1"><a class="header-anchor" href="#输入-4"><span>输入</span></a></h2><p><code>destination</code></p><p>: 将要被粘贴的遮罩。</p><p><code>source</code></p><p>: 要粘贴的遮罩。</p><p><code>x</code></p><p>: 粘贴遮罩的x坐标，单位为像素。</p><p><code>y</code></p><p>: 粘贴遮罩的y坐标，单位为像素。</p><p><code>operation</code></p><p>: 粘贴遮罩的方式。</p><h2 id="输出-4" tabindex="-1"><a class="header-anchor" href="#输出-4"><span>输出</span></a></h2><p><code>MASK</code></p><p>: 包含粘贴到<code>destination</code>中的<code>source</code>的新遮罩合成。</p><h2 id="示例-5" tabindex="-1"><a class="header-anchor" href="#示例-5"><span>示例</span></a></h2><p>遮罩合成节点非常适用于需要将一个遮罩层叠到另一个上的场景，尤其是当您想要在图像的特定部分应用不同的效果或调整时。以下是一个简单的使用案例：</p><p>在此工作流中，我们有两个遮罩 - 一个是主遮罩（destination），另一个我们希望粘贴到主遮罩上的次遮罩（source）。</p><ol><li>将主遮罩连接到<code>destination</code>输入。</li><li>将要粘贴的遮罩连接到<code>source</code>输入。</li><li>在<code>x</code>和<code>y</code>输入中设置坐标，确定次遮罩在主遮罩上的位置。</li><li>选择<code>operation</code>以确定粘贴时如何合并遮罩。例如，“正常”会简单地将次遮罩放在主遮罩上，“相加”会合并遮罩的亮度值等。</li></ol><p>输出的遮罩将是次遮罩粘贴到主遮罩上的结果，根据所选择的操作方式，可能会有不同的视觉效果。</p><p>此节点非常适用于复杂的图像处理任务，其中需要精确控制应用于图像不同部分的效果。</p><p>这种方法提供了一种高度可定制的方式来合并遮罩，为图像编辑提供更多的灵活性和控制。</p><h3 id="纯色遮罩-solid-mask" tabindex="-1"><a class="header-anchor" href="#纯色遮罩-solid-mask"><span>纯色遮罩 Solid Mask</span></a></h3><p><img src="'+Le+'" alt="Solid Mask node"></p><p>纯色遮罩节点可以用来创建一个填充了单一值的纯色遮罩。</p><h2 id="输入-5" tabindex="-1"><a class="header-anchor" href="#输入-5"><span>输入</span></a></h2><p><code>value</code></p><p>: 填充遮罩的值。</p><p><code>width</code></p><p>: 遮罩的宽度。</p><p><code>height</code></p><p>: 遮罩的高度。</p><h2 id="输出-5" tabindex="-1"><a class="header-anchor" href="#输出-5"><span>输出</span></a></h2><p><code>MASK</code></p><p>: 填充了单一值的遮罩。</p><h2 id="示例-6" tabindex="-1"><a class="header-anchor" href="#示例-6"><span>示例</span></a></h2><p>纯色遮罩节点在您需要为整个图像或特定区域创建统一遮罩时非常有用，尤其是在需要遮蔽背景或仅对图像的特定部分应用效果时。以下是一个简单的使用案例：</p><ol><li>将您想要填充的值输入到<code>value</code>中，该值范围从0（完全透明）到1（完全不透明）。</li><li>输入您期望的遮罩<code>width</code>和<code>height</code>，通常这会与您工作的图像尺寸相匹配。</li><li>节点会生成一个填充了所选值的纯色遮罩，可以被应用到其他节点，例如遮罩合成节点或KSampler节点。</li></ol><p>这个纯色遮罩可以用作其他遮罩操作的基础，或者直接用来控制图像的哪些部分应该被采样器处理或保留。</p><h2 id="采样-sampling" tabindex="-1"><a class="header-anchor" href="#采样-sampling"><span>采样 Sampling</span></a></h2><p>采样节点提供了一种使用扩散模型对潜在图像进行去噪的方法。</p><h3 id="ksampler" tabindex="-1"><a class="header-anchor" href="#ksampler"><span>KSampler</span></a></h3><p><img src="'+Ie+'" alt="KSampler 节点"></p><p>KSampler 使用提供的模型以及正向和负向条件来生成给定潜像的新版本。首先，根据给定的 <code>seed</code> 和 <code>denoise</code> 强度对潜像进行加噪，部分擦除潜像。然后使用给定的 <code>Model</code> 以及 <code>positive</code> 和 <code>negative</code> 条件作为指导，去除这些噪声，在噪声擦除图像的地方“构想”新的细节。</p><h2 id="输入-6" tabindex="-1"><a class="header-anchor" href="#输入-6"><span>输入</span></a></h2><p><code>Model</code></p><p>: 用于去噪的模型</p><p><code>Positive</code></p><p>: 正面调节。</p><p><code>Negative</code></p><p>: 负面调节。</p><p><code>latent_image</code></p><p>: 将被去噪的潜像。</p><p><code>seed</code></p><p>: 用于创建噪声的随机种子。</p><p><code>control_after_generate</code></p><p>: 提供在每个提示后更改上述种子号的能力。节点可以<code>randomize</code>、<code>increment</code>、<code>decrement</code>或保持种子号<code>fixed</code>。</p><p><code>steps</code></p>',171),Se=e("p",null,[e("code",null,"cfg")],-1),Pe=e("p",null,": 无分类器指导(cfg)比例决定了采样器在实现提示内容到最终图像中应该有多积极。更高的比例迫使图像更好地代表提示，但设置过高的比例会对图像的质量产生负面影响。",-1),Re=e("p",null,[e("code",null,"sampler_name")],-1),Fe=e("p",null,[e("code",null,"scheduler")],-1),Ze=n('<p><code>denoise</code></p><p>: 应该有多少潜像信息被噪声擦除。</p><h2 id="输出-6" tabindex="-1"><a class="header-anchor" href="#输出-6"><span>输出</span></a></h2><p><code>LATENT</code></p><p>: 去噪后的潜像。</p><h2 id="示例-7" tabindex="-1"><a class="header-anchor" href="#示例-7"><span>示例</span></a></h2><p>KSampler 是任何工作流的核心，可用于执行文本到图像和图像到图像的生成任务。下面的示例展示了如何在图像到图像任务中使用 KSampler，通过连接一个模型、一个正面和负面嵌入以及一个潜像。注意，我们使用的去噪值小于1.0。这样，在对原始图像进行加噪时，原始图像的部分内容得以保留，指导去噪过程生成相似的图像。</p><h3 id="ksampler-高级" tabindex="-1"><a class="header-anchor" href="#ksampler-高级"><span>KSampler 高级</span></a></h3><p><img src="'+be+'" alt="KSampler Advanced节点"></p>',9),Qe=e("code",null,"add_noise",-1),Be=e("code",null,"return_with_leftover_noise",-1),De=e("code",null,"denoise",-1),We=e("code",null,"start_at_step",-1),Ge=e("code",null,"end_at_step",-1),ze=n('<p>!!! 提示</p><pre><code>假设`end_at_step &gt;= steps`，KSampler Advanced节点将以与KSampler节点具有以下`denoise`设置的完全相同的方式去噪潜像：\n\n`denoise = (steps - start_at_step) / steps`\n</code></pre><h2 id="输入-7" tabindex="-1"><a class="header-anchor" href="#输入-7"><span>输入</span></a></h2><p><code>Model</code></p><p>: 用于去噪的模型</p><p><code>Positive</code></p><p>: 正面调节。</p><p><code>Negative</code></p><p>: 负面调节。</p><p><code>latent_image</code></p><p>: 将被去噪的潜像。</p><p><code>add_noise</code></p><p>: 是否在去噪前向潜像中添加噪声。启用时，节点将为给定的起始步骤注入适当的噪声。</p><p><code>seed</code></p><p>: 用于创建噪声的随机种子。</p><p><code>control_after_generate</code></p><p>: 提供在每个提示后更改上述种子号的能力。节点可以<code>randomize</code>、<code>increment</code>、<code>decrement</code>或保持种子号<code>fixed</code>。</p><p><code>steps</code></p>',18),Oe=e("p",null,[e("code",null,"cfg")],-1),je=e("p",null,": 无分类器指导(cfg)比例决定了采样器在实现提示内容到最终图像中应该有多积极。更高的比例迫使图像更好地代表提示，但设置过高的比例会对图像的质量产生负面影响。",-1),He=e("p",null,[e("code",null,"sampler_name")],-1),Je=e("p",null,[e("code",null,"scheduler")],-1),Ye=n('<p><code>start_at_step</code></p><p>: 确定在计划的哪一步开始去噪过程。</p><p><code>end_at_step</code></p><p>: 确定在哪一步结束去噪。当此设置超过<code>steps</code>时，计划将在<code>steps</code>处结束</p><p><code>return_with_leftover_noise</code></p><p>: 禁用时，KSampler Advanced将尝试在最后一步完全去除潜像中的噪声。根据此操作跳过计划中的多少步骤，输出可能不准确且质量较低。</p><h2 id="输出-7" tabindex="-1"><a class="header-anchor" href="#输出-7"><span>输出</span></a></h2><p><code>LATENT</code></p><p>: 去噪后的潜像。</p>',9);function Xe(qe,Ke){const s=l("ExternalLinkIcon"),t=l("RouteLink");return i(),c("div",null,[we,e("blockquote",null,[e("p",null,[o("还值得一提的是，Clip set last layer 仅适用于使用CLIP或基于使用CLIP的模型，如1.x模型及其衍生模型。较新的模型，如2.0模型，使用OpenCLIP而不是CLIP，并且与CLIP的交互方式不同。"),e("a",_e,[o("知识库"),p(s)]),o(" ​")])]),Ce,xe,Te,e("blockquote",null,[e("p",null,[o("有关ComfyUI中所有与文本提示相关的功能的完整指南，"),e("a",ye,[o("请参阅此页面"),p(s)]),o("。")])]),Ee,e("p",null,[o("保存图像：Save Image节点可用于保存图像。要在节点图中简单预览图，请使用Preview Image节点。要更好地组织生成的所有图像，可以将特殊格式的字符串传递给带有file_prefix小部件的输出节点。有关如何格式化字符串的更多信息，"),e("a",Ne,[o("请参阅此页面"),p(s)])]),Me,e("p",null,[o("The Upscale Image node can be used to resize pixel images. To upscale images using AI see the "),p(t,{to:"/posts/tutorial/core_nodes/UpscaleImageUsingModel.html"},{default:a(()=>[o("Upscale Image Using Model")]),_:1}),o(" node.")]),Ve,e("p",null,[o("The Load VAE node can be used to load a specific VAE model, VAE models are used to encoding and decoding images to and from latent space. Although the "),p(t,{to:"/posts/tutorial/core_nodes/LoadCheckpoint.html"},{default:a(()=>[o("Load Checkpoint")]),_:1}),o(" node provides a VAE model alongside the diffusion model, sometimes it can be useful to use a specific VAE model.")]),Ue,e("p",null,[o("At times you might wish to use a different VAE than the one that came loaded with the "),p(t,{to:"/posts/tutorial/core_nodes/LoadCheckpoint.html"},{default:a(()=>[o("Load Checkpoint")]),_:1}),o(" node. In the example below we use a different VAE to encode an image to latent space, and decode the result of the Ksampler.")]),Ae,e("p",null,[o(": 去噪期间使用的步骤数。允许采样器进行的步骤越多，结果就越准确。有关如何选择适当步骤数的好的指导方针，请参见"),p(t,{to:"/posts/tutorial/core_nodes/samplers.html"},{default:a(()=>[o("samplers")]),_:1}),o("页面。")]),Se,Pe,Re,e("p",null,[o(": 要使用的采样器，有关可用采样器的更多详细信息，请参见"),p(t,{to:"/posts/tutorial/core_nodes/samplers.html"},{default:a(()=>[o("samplers")]),_:1}),o("页面。")]),Fe,e("p",null,[o(": 要使用的计划类型，有关可用计划的更多详细信息，请参见"),p(t,{to:"/posts/tutorial/core_nodes/samplers.html"},{default:a(()=>[o("samplers")]),_:1}),o("页面。")]),Ze,e("p",null,[o("KSampler Advanced节点是"),p(t,{to:"/posts/tutorial/core_nodes/KSampler.html"},{default:a(()=>[o("KSampler")]),_:1}),o("节点的更高级版本。尽管KSampler节点总是向潜像中添加噪声，然后完全去除加噪后的潜像，但KSampler Advanced节点提供了额外的设置来控制这种行为。可以通过"),Qe,o("设置告诉KSampler Advanced节点不向潜像中添加噪声。它还可以通过"),Be,o("设置返回部分去噪后的图像。与KSampler节点不同，此节点没有"),De,o("设置，但这个过程是由"),We,o("和"),Ge,o("设置控制的。这使得例如将部分去噪的潜在图像交给另一个KSampler Advanced节点来完成过程成为可能。")]),ze,e("p",null,[o(": 计划中的步骤数。允许采样器进行的步骤越多，结果就越准确。有关如何选择适当步骤数的好的指导方针，请参见"),p(t,{to:"/posts/tutorial/core_nodes/samplers.html"},{default:a(()=>[o("samplers")]),_:1}),o("页面。")]),Oe,je,He,e("p",null,[o(": 要使用的采样器，有关可用采样器的更多详细信息，请参见"),p(t,{to:"/posts/tutorial/core_nodes/samplers.html"},{default:a(()=>[o("samplers")]),_:1}),o("页面。")]),Je,e("p",null,[o(": 要使用的计划类型，有关可用计划的更多详细信息，请参见"),p(t,{to:"/posts/tutorial/core_nodes/samplers.html"},{default:a(()=>[o("samplers")]),_:1}),o("页面。")]),Ye])}const eo=d(ve,[["render",Xe],["__file","index.html.vue"]]),oo=JSON.parse('{"path":"/posts/tutorial/core_nodes/","title":"核心节点","lang":"en-US","frontmatter":{"description":"核心节点 扩散模型加载器 Diffusers Loader节点（扩散模型加载器），可用于加载扩散模型。 图片 输入 model_path：扩散器模型的路径 输出 MODEL：用于去噪潜变量的模型。 CLIP：用于编码文本提示的CLIP模型。 VAE：用于将图像编码和解码到潜空间的VAE模型。 加载检查点节点 Load Checkpoint (With ...","head":[["meta",{"property":"og:url","content":"https://www.mixcomfy.com/awesome-comfyui-workflow/posts/tutorial/core_nodes/"}],["meta",{"property":"og:site_name","content":"ComfyUI中文爱好者社区"}],["meta",{"property":"og:title","content":"核心节点"}],["meta",{"property":"og:description","content":"核心节点 扩散模型加载器 Diffusers Loader节点（扩散模型加载器），可用于加载扩散模型。 图片 输入 model_path：扩散器模型的路径 输出 MODEL：用于去噪潜变量的模型。 CLIP：用于编码文本提示的CLIP模型。 VAE：用于将图像编码和解码到潜空间的VAE模型。 加载检查点节点 Load Checkpoint (With ..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"en-US"}],["meta",{"property":"og:updated_time","content":"2024-04-20T12:53:10.000Z"}],["meta",{"property":"article:modified_time","content":"2024-04-20T12:53:10.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"核心节点\\",\\"image\\":[\\"\\"],\\"dateModified\\":\\"2024-04-20T12:53:10.000Z\\",\\"author\\":[]}"]]},"headers":[{"level":2,"title":"扩散模型加载器","slug":"扩散模型加载器","link":"#扩散模型加载器","children":[]},{"level":2,"title":"加载检查点节点","slug":"加载检查点节点","link":"#加载检查点节点","children":[]},{"level":2,"title":"条件设定","slug":"条件设定","link":"#条件设定","children":[]},{"level":2,"title":"应用ControlNet模型 Apply ControlNet","slug":"应用controlnet模型-apply-controlnet","link":"#应用controlnet模型-apply-controlnet","children":[]},{"level":2,"title":"应用风格模型","slug":"应用风格模型","link":"#应用风格模型","children":[]},{"level":2,"title":"设置CLIP最后一层","slug":"设置clip最后一层","link":"#设置clip最后一层","children":[]},{"level":2,"title":"文本提示","slug":"文本提示","link":"#文本提示","children":[]},{"level":2,"title":"视觉编码","slug":"视觉编码","link":"#视觉编码","children":[]},{"level":2,"title":"平均条件化","slug":"平均条件化","link":"#平均条件化","children":[]},{"level":2,"title":"Conditioning (Combine)","slug":"conditioning-combine","link":"#conditioning-combine","children":[]},{"level":2,"title":"Conditioning (Set Area)","slug":"conditioning-set-area","link":"#conditioning-set-area","children":[{"level":3,"title":"Conditioning (Set Mask)","slug":"conditioning-set-mask","link":"#conditioning-set-mask","children":[]}]},{"level":2,"title":"example","slug":"example","link":"#example","children":[{"level":3,"title":"GLIGEN Textbox Apply","slug":"gligen-textbox-apply","link":"#gligen-textbox-apply","children":[]}]},{"level":2,"title":"example","slug":"example-1","link":"#example-1","children":[]},{"level":2,"title":"实验性 Experimental","slug":"实验性-experimental","link":"#实验性-experimental","children":[{"level":3,"title":"Load Latent","slug":"load-latent","link":"#load-latent","children":[]},{"level":3,"title":"Save Latent","slug":"save-latent","link":"#save-latent","children":[]},{"level":3,"title":"Tome Patch Model","slug":"tome-patch-model","link":"#tome-patch-model","children":[]}]},{"level":2,"title":"TokenMerging for Stable Diffusion","slug":"tokenmerging-for-stable-diffusion","link":"#tokenmerging-for-stable-diffusion","children":[{"level":3,"title":"VAE Decode (Tiled)","slug":"vae-decode-tiled","link":"#vae-decode-tiled","children":[]},{"level":3,"title":"VAE Encode (Tiled)","slug":"vae-encode-tiled","link":"#vae-encode-tiled","children":[]}]},{"level":2,"title":"图像 Image","slug":"图像-image","link":"#图像-image","children":[{"level":3,"title":"Load Image","slug":"load-image","link":"#load-image","children":[]},{"level":3,"title":"示例","slug":"示例","link":"#示例","children":[]},{"level":3,"title":"Invert Image","slug":"invert-image","link":"#invert-image","children":[]},{"level":3,"title":"Pad Image for Outpainting","slug":"pad-image-for-outpainting","link":"#pad-image-for-outpainting","children":[]},{"level":3,"title":"Preview Image","slug":"preview-image","link":"#preview-image","children":[]},{"level":3,"title":"Save Image","slug":"save-image","link":"#save-image","children":[]},{"level":3,"title":"Image Blend#postprocessing","slug":"image-blend-postprocessing","link":"#image-blend-postprocessing","children":[]},{"level":3,"title":"Image Blur","slug":"image-blur","link":"#image-blur","children":[]}]},{"level":2,"title":"Image Quantize","slug":"image-quantize","link":"#image-quantize","children":[]},{"level":2,"title":"Image Sharpen","slug":"image-sharpen","link":"#image-sharpen","children":[]},{"level":2,"title":"Upscale Image","slug":"upscale-image","link":"#upscale-image","children":[]},{"level":2,"title":"Upscale Image (using Model)","slug":"upscale-image-using-model","link":"#upscale-image-using-model","children":[]},{"level":2,"title":"Latent 潜在空间","slug":"latent-潜在空间","link":"#latent-潜在空间","children":[{"level":3,"title":"Empty Latent Image","slug":"empty-latent-image","link":"#empty-latent-image","children":[]},{"level":3,"title":"Latent Composite","slug":"latent-composite","link":"#latent-composite","children":[]},{"level":3,"title":"Latent Composite Masked","slug":"latent-composite-masked","link":"#latent-composite-masked","children":[]},{"level":3,"title":"Upscale Latent","slug":"upscale-latent","link":"#upscale-latent","children":[]},{"level":3,"title":"VAE Decode","slug":"vae-decode","link":"#vae-decode","children":[]}]},{"level":2,"title":"example","slug":"example-2","link":"#example-2","children":[{"level":3,"title":"VAE Encode","slug":"vae-encode","link":"#vae-encode","children":[]}]},{"level":2,"title":"示例","slug":"示例-1","link":"#示例-1","children":[{"level":3,"title":"Latent From Batch","slug":"latent-from-batch","link":"#latent-from-batch","children":[]},{"level":3,"title":"Rebatch Latents","slug":"rebatch-latents","link":"#rebatch-latents","children":[]},{"level":3,"title":"Repeat Latent Batch","slug":"repeat-latent-batch","link":"#repeat-latent-batch","children":[]},{"level":3,"title":"Set Latent Noise Mask","slug":"set-latent-noise-mask","link":"#set-latent-noise-mask","children":[]},{"level":3,"title":"VAE Encode (for Inpainting)","slug":"vae-encode-for-inpainting","link":"#vae-encode-for-inpainting","children":[]},{"level":3,"title":"Crop Latent","slug":"crop-latent","link":"#crop-latent","children":[]},{"level":3,"title":"Flip Latent","slug":"flip-latent","link":"#flip-latent","children":[]},{"level":3,"title":"Rotate Latent","slug":"rotate-latent","link":"#rotate-latent","children":[]}]},{"level":2,"title":"载入器 Loaders","slug":"载入器-loaders","link":"#载入器-loaders","children":[{"level":3,"title":"GLIGEN Loader","slug":"gligen-loader","link":"#gligen-loader","children":[]},{"level":3,"title":"Hypernetwork Loader","slug":"hypernetwork-loader","link":"#hypernetwork-loader","children":[]}]},{"level":2,"title":"inputs","slug":"inputs","link":"#inputs","children":[]},{"level":2,"title":"outputs","slug":"outputs","link":"#outputs","children":[{"level":3,"title":"Load Checkpoint","slug":"load-checkpoint","link":"#load-checkpoint","children":[]}]},{"level":2,"title":"inputs","slug":"inputs-1","link":"#inputs-1","children":[]},{"level":2,"title":"outputs","slug":"outputs-1","link":"#outputs-1","children":[{"level":3,"title":"Load CLIP","slug":"load-clip","link":"#load-clip","children":[]}]},{"level":2,"title":"inputs","slug":"inputs-2","link":"#inputs-2","children":[]},{"level":2,"title":"outputs","slug":"outputs-2","link":"#outputs-2","children":[{"level":3,"title":"Load CLIP Vision","slug":"load-clip-vision","link":"#load-clip-vision","children":[]}]},{"level":2,"title":"inputs","slug":"inputs-3","link":"#inputs-3","children":[]},{"level":2,"title":"outputs","slug":"outputs-3","link":"#outputs-3","children":[{"level":3,"title":"Load ControlNet Model","slug":"load-controlnet-model","link":"#load-controlnet-model","children":[]}]},{"level":2,"title":"inputs","slug":"inputs-4","link":"#inputs-4","children":[]},{"level":2,"title":"outputs","slug":"outputs-4","link":"#outputs-4","children":[{"level":3,"title":"Load LoRA","slug":"load-lora","link":"#load-lora","children":[]}]},{"level":2,"title":"inputs","slug":"inputs-5","link":"#inputs-5","children":[]},{"level":2,"title":"outputs","slug":"outputs-5","link":"#outputs-5","children":[{"level":3,"title":"Load Style Model","slug":"load-style-model","link":"#load-style-model","children":[]}]},{"level":2,"title":"inputs","slug":"inputs-6","link":"#inputs-6","children":[]},{"level":2,"title":"outputs","slug":"outputs-6","link":"#outputs-6","children":[{"level":3,"title":"Load Upscale Model","slug":"load-upscale-model","link":"#load-upscale-model","children":[]}]},{"level":2,"title":"inputs","slug":"inputs-7","link":"#inputs-7","children":[]},{"level":2,"title":"outputs","slug":"outputs-7","link":"#outputs-7","children":[{"level":3,"title":"Load VAE","slug":"load-vae","link":"#load-vae","children":[]}]},{"level":2,"title":"inputs","slug":"inputs-8","link":"#inputs-8","children":[]},{"level":2,"title":"outputs","slug":"outputs-8","link":"#outputs-8","children":[]},{"level":2,"title":"example","slug":"example-3","link":"#example-3","children":[{"level":3,"title":"unCLIP Checkpoint Loader","slug":"unclip-checkpoint-loader","link":"#unclip-checkpoint-loader","children":[]}]},{"level":2,"title":"inputs","slug":"inputs-9","link":"#inputs-9","children":[]},{"level":2,"title":"outputs","slug":"outputs-9","link":"#outputs-9","children":[]},{"level":2,"title":"遮罩 Mask","slug":"遮罩-mask","link":"#遮罩-mask","children":[{"level":3,"title":"将图像转换为遮罩","slug":"将图像转换为遮罩","link":"#将图像转换为遮罩","children":[]}]},{"level":2,"title":"示例","slug":"示例-2","link":"#示例-2","children":[{"level":3,"title":"裁剪遮罩","slug":"裁剪遮罩","link":"#裁剪遮罩","children":[]}]},{"level":2,"title":"输入","slug":"输入","link":"#输入","children":[]},{"level":2,"title":"输出","slug":"输出","link":"#输出","children":[]},{"level":2,"title":"示例","slug":"示例-3","link":"#示例-3","children":[{"level":3,"title":"羽化遮罩","slug":"羽化遮罩","link":"#羽化遮罩","children":[]}]},{"level":2,"title":"输入","slug":"输入-1","link":"#输入-1","children":[]},{"level":2,"title":"输出","slug":"输出-1","link":"#输出-1","children":[]},{"level":2,"title":"示例","slug":"示例-4","link":"#示例-4","children":[{"level":3,"title":"反转遮罩","slug":"反转遮罩","link":"#反转遮罩","children":[]}]},{"level":2,"title":"输入","slug":"输入-2","link":"#输入-2","children":[]},{"level":2,"title":"输出","slug":"输出-2","link":"#输出-2","children":[]},{"level":2,"title":"实例","slug":"实例","link":"#实例","children":[{"level":3,"title":"加载图片（作为遮罩）","slug":"加载图片-作为遮罩","link":"#加载图片-作为遮罩","children":[]}]},{"level":2,"title":"输入","slug":"输入-3","link":"#输入-3","children":[]},{"level":2,"title":"输出","slug":"输出-3","link":"#输出-3","children":[]},{"level":2,"title":"实例","slug":"实例-1","link":"#实例-1","children":[{"level":3,"title":"遮罩合成","slug":"遮罩合成","link":"#遮罩合成","children":[]}]},{"level":2,"title":"输入","slug":"输入-4","link":"#输入-4","children":[]},{"level":2,"title":"输出","slug":"输出-4","link":"#输出-4","children":[]},{"level":2,"title":"示例","slug":"示例-5","link":"#示例-5","children":[{"level":3,"title":"纯色遮罩 Solid Mask","slug":"纯色遮罩-solid-mask","link":"#纯色遮罩-solid-mask","children":[]}]},{"level":2,"title":"输入","slug":"输入-5","link":"#输入-5","children":[]},{"level":2,"title":"输出","slug":"输出-5","link":"#输出-5","children":[]},{"level":2,"title":"示例","slug":"示例-6","link":"#示例-6","children":[]},{"level":2,"title":"采样 Sampling","slug":"采样-sampling","link":"#采样-sampling","children":[{"level":3,"title":"KSampler","slug":"ksampler","link":"#ksampler","children":[]}]},{"level":2,"title":"输入","slug":"输入-6","link":"#输入-6","children":[]},{"level":2,"title":"输出","slug":"输出-6","link":"#输出-6","children":[]},{"level":2,"title":"示例","slug":"示例-7","link":"#示例-7","children":[{"level":3,"title":"KSampler 高级","slug":"ksampler-高级","link":"#ksampler-高级","children":[]}]},{"level":2,"title":"输入","slug":"输入-7","link":"#输入-7","children":[]},{"level":2,"title":"输出","slug":"输出-7","link":"#输出-7","children":[]}],"git":{"updatedTime":1713617590000,"contributors":[{"name":"shadowcz007","email":"chizhiwei007@163.com","commits":1}]},"filePathRelative":"posts/tutorial/core_nodes/index.md","autoDesc":true,"excerpt":"\\n<h2>扩散模型加载器</h2>\\n<blockquote>\\n<p>Diffusers Loader节点（扩散模型加载器），可用于加载扩散模型。</p>\\n</blockquote>\\n<p></p>\\n<p><strong>输入</strong></p>\\n<ul>\\n<li><code>model_path</code>：扩散器模型的路径</li>\\n</ul>\\n<p><strong>输出</strong></p>\\n<ul>\\n<li>\\n<p><code>MODEL</code>：用于去噪潜变量的模型。</p>\\n</li>\\n<li>\\n<p><code>CLIP</code>：用于编码文本提示的CLIP模型。</p>\\n</li>\\n<li>\\n<p><code>VAE</code>：用于将图像编码和解码到潜空间的VAE模型。</p>\\n</li>\\n</ul>"}');export{eo as comp,oo as data};
